---
title: "<center>US Health Insurance Premiums Analysis</center>"
author: "<center>Christopher Prasetya Mulya</center>"
date: "<center>2023-12-19</center>"
output:
  html_document: default
  pdf_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(tree)
library(glmnet)
library(DAAG)
library(gbm)
library(caret)
```

## Introduction

This analysis aims to model individuals health insurance charges in the
United States based on several demographic indicators, namely: BMI,
Region, Smoker, Children, Sex, and Age.

## Setup

Splitting randomized data into 2/3 train and 1/3 test set.

```{r question_3_data_setup}

set.seed(4)
insurance_data <- read.csv("insurance.csv", stringsAsFactors=TRUE)
print(summary(insurance_data))

data_randomized <- sample(insurance_data)

#setting up training and testing set
d_train <- data_randomized[1:(2*length(data_randomized$charges)/3),]
d_test <- data_randomized[(2*length(data_randomized$charges)/3+1):(length(data_randomized$charges)),]

```

### Linear Model

```{r question_3_linear}

lm_insu_reg <- lm(charges~., data=d_train)
summary(lm_insu_reg)

par(mfrow = c(2, 2))
plot(lm_insu_reg)
par(mfrow = c(1, 1))

cvlm <- list()
msecv <- NA
for(i in 1:nrow(d_train)){
  cvlm[[i]] <- lm(charges~., data=d_train[-i,])
  msecv[i] <- (predict(cvlm[[i]], newdata=d_train[i,]) - d_train$charges[[i]])^2
}
lm_cv_MSE <- mean(msecv)
sprintf("CV MSE estimate for linear model on training data: %s", round(lm_cv_MSE, digits=3))

predicted_vals_test <- predict(lm_insu_reg, newdata = d_test)
lm_test_MSE <- mean((d_test$charges - predicted_vals_test)^2)
sprintf("MSE estimate for linear model on testing data: %s", round(lm_test_MSE, digits=3))
```

Based on the regression plots, the residuals don't seem independent or
normally distributed or have constant variance which violate many of the
assumptions necessary to fit a linear model.

### Lasso

```{r question_3_lasso, warning=FALSE}

x <- as.matrix(d_train[, -2])
y <- as.vector(d_train[, 2])


lasso_reg <- cv.glmnet(x,y, alpha = 1, lamda = lambda_values)
plot(lasso_reg)
lasso_cv_mse <- min(lasso_reg$cvm)
print(lasso_cv_mse)
best_lasso_reg <- glmnet(x, y, alpha = 1, lambda = lasso_reg$lambda.min)

predicted_vals_test <- predict(best_lasso_reg, newx = as.matrix(d_test[,-2]))
lasso_test_MSE <- mean((d_test$charges - predicted_vals_test)^2)
sprintf("MSE estimate for Lasso regression on testing data: %s", round(lasso_test_MSE, digits=3))

```

### Trees

```{r question_3_trees}

tree_insu_reg <- tree(charges~., data = d_train)

set.seed(51341)
tree_insu_cv <- cv.tree(tree_insu_reg)
plot(tree_insu_cv, type="b") 
print("No pruning needed")

tree_cv_MSE <- min(tree_insu_cv$dev)/length(insurance_data$charges)
sprintf("Tree CV MSE estimate based on training data: %s", round(tree_cv_MSE,digits=3)) # Tree CV MSE Estimate

plot(tree_insu_reg)
text(tree_insu_reg, pretty=0)

summary(tree_insu_reg)

predicted_vals_test <- predict(tree_insu_reg, newdata = d_test)
tree_test_MSE <- mean((d_test$charges - predicted_vals_test)^2)

sprintf("Tree MSE estimate based on testing data: %s", round(tree_test_MSE,digits=3))
```

### Random Forest

```{r question_3_random_forest}
RF_insu_reg <- randomForest(charges~., data=d_train, mtry=4, importance=TRUE)
print(RF_insu_reg)
varImpPlot(RF_insu_reg)
RF_cv_MSE <- RF_insu_reg$mse[500]
sprintf("Random forest MSE estimate based on OOB estimates from training data: %s",RF_cv_MSE)

predicted_vals_test <- predict(RF_insu_reg, newdata = d_test)
RF_test_MSE <- mean((d_test$charges - predicted_vals_test)^2)

sprintf("Tree MSE estimate based on testing data: %s", round(RF_test_MSE,digits=3))
```

### Boosting

```{r question_3_boosting}

boost_insu_reg <- gbm(charges~., distribution="gaussian", data=d_train, n.trees=5000, interaction.depth=4, shrinkage = 0.001)

# Create a training control object for cross-validation
ctrl <- trainControl(method = "cv", number = 20)  # 5-fold cross-validation

# Use the train function from caret for cross-validation
cv_results <- train(charges ~ ., data = d_train, method = "gbm", trControl = ctrl, verbose = FALSE)

# Access the cross-validated error values
boost_cv_MSE <- mean(cv_results$results$RMSE^2)
sprintf("Boosted CV MSE estimate based on training data: %s",boost_cv_MSE)

predicted_vals_test <- predict(boost_insu_reg, newdata = d_test)
boost_test_MSE <- mean((d_test$charges - predicted_vals_test)^2)
sprintf("Boosted MSE estimate based on testing data: %s",boost_test_MSE)

```

| Model         |      CV MSE      |      Test MSE      |
|:--------------|:----------------:|:------------------:|
| Linear Model  |  `r lm_cv_MSE`   |  `r lm_test_MSE`   |
| Lasso         | `r lasso_cv_mse` | `r lasso_test_MSE` |
| Trees         | `r tree_cv_MSE`  | `r tree_test_MSE`  |
| Random Forest |  `r RF_cv_MSE`   |  `r RF_test_MSE`   |
| Boosting      | `r boost_cv_MSE` | `r boost_test_MSE` |

Based on the above metrics for test MSE, the model that is most likely
to provide the lowest MSE in the long run is the Boosting (Gradient
Boosting) model, as it has the lowest estimated MSE on the test set
(21811530). The lower MSE suggests that, on average, the Boosting model
is providing better predictions on unseen data compared to the other
models in this specific analysis.

However, in consulting with an insurance company on this dataset, I
would choose the Decision Tree model. The Decision Tree's inherent
simplicity and interpretability make it an ideal choice for
stakeholders, including vendors and non-technical personnel, to easily
comprehend and trust the decision-making process. In the context of
health insurance charges, a Decision Tree provides a clear and
transparent representation of the factors influencing the predictions,
facilitating straightforward understanding of how demographic features
impact charges. While other advanced models like Random Forests or
Boosting may offer improved predictive accuracy, the Decision Tree
strikes a balance by providing a concise and intuitive model that aligns
well with the need for transparent decision-making in the insurance
industry, ultimately enhancing the model's acceptance and utility within
the organization.
